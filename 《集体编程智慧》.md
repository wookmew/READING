# 第二章 提供推荐  
-----

机器学习就是允许计算机不断地进行学习，对数据中重要的特征进行归纳，并借此得到一个模型。<br/>
协作型过滤：一般是对一大群人进行搜索，并从中找出品味相近的人。<br/>
<font size=5>**基于用户的协作型过滤**</font><br/>
**1. 搜集偏好**<br/>
**2. 寻找相近用户**<br/>
    - 欧几里德距离<br/>
        即一个多维度的坐标轴，将数据绘制到坐标轴上，比较其相关距离。<br/>
    - 皮尔逊相关度<br/>
        该相关系数是判断两组数据和某一直线拟合程度的一种度量。<br/>
**3. 为评论者打分**<br/>
**4. 推荐物品**<br/>
**5. 匹配商品**<br/>  
<font size=5>**基于物品的协作型过滤**</font><br/>
**总体思路**:为每件物品预先计算好最为相近的其他物品，当为某位用户提供推荐时，可以查看他曾经评过分的物品。从中挑选出排位靠前的，再构造出一个加权列表。(物品间比较不会像用户那样频繁。换个角度看待数据，求一本书的评分，可以书当key，而人们的评分当value。基于物品推荐比基于人物推荐更快。<br/>
**1. 构造物品比较数据集**<br/>
      找到数据的相似性参数，在考虑算法，比如书中举例，2维数组，求和，求方差，求方差和等等。<br/>
**2. 获得推荐**<br/>

# 第三章 获得群组  
---

**数据聚类**：寻找紧密相关的人，实物或观点，将其可视化的方法。 <br/>
**聚类算法**：目标是采集数据，然后从中找出不同的群组。<br/> 
**分类聚类**：不断的将最相似的群组两两合并，构造出一个群组的层级结构。树状图就是一种分类聚类。 <br/>
**列聚类**：对行或者列上的数据进行聚类，一般最终会转化为矩阵或者树状图。 <br/>
**K-均值聚类算法** <br/>:先随机确定K个中心位置，然后将将各个数据项分配给最临近的中心点。分配完成后，聚类中心会移动到分配给该聚类的所有节点的平均位置处。然后继续分配，直到此过程不再产生变化。<br/>
多维缩放，为数据集找到一种二维表达形式。

# 第四章 搜索与排名
---

**搜索引擎** == 爬虫 + 索引 + 检索，检索是最为奥妙的地方。<br/>
**1. 爬虫的设计**  <br/>
即找到一个搜集文档的方法。<br/>
**2. 建立索引**  <br/>
索引对应于一个列表，其中包含了所有不同的单词，这些单词所在的文档，以及单次在文档中出现的位置。<br/>
**3. 建立数据库**  <br/>
    实现索引功能<br/>
**4. 在网页中查找单词  <br/>
5. 加入索引  <br/>
6. 查询  <br/>
7. 基于内容的排名**  <br/>
    - 单词频度  <br/>
      位于擦汗寻条件中的单词在文档中出现的次数能帮助我们判断文档的相关程度。<br/>
    - 文档位置  <br/>
      文档的主题可能会出现在靠近文档的开始处。<br/>
    - 单次距离  <br/>
      如果查询条件中有多个单词，则他们在文档中出现的位置应该靠得很近。<br/>
**8. 归一化函数**  <br/>
    对于不同方法的返回结果进行比较，我们需要一种对结果进行归一化处理的办法。即，令它们具有相同的值域或变化方向。<br/>
**9. 单词频度**  <br/>
    根据查询条件中的单次在网页中出现的次数对网页进行评价。<br/>
**10. 文档位置**  <br/>
    即搜索单词在网页中的位置。<br/>
**11. 单词距离**  <br/>
  
<font size=5>**利用外部回指链接**</font>
**1. 简单计数**  <br/>
      在每个网页上统计链接数目，并将链接总数作为针对网页的度量。<br/>
**2. PageRank算法**  <br/>
      PageRank通过网络浩瀚的超链接关系来确定一个页面的等级。Google把从A页面到B页面的链接解释为A页面给B页面投票，Google根据投票来源（甚至来源的来源，即链接到A页面的页面）和投票目标的等级来决定新的等级。简单的说，一个高等级的页面可以使其他低等级页面的等级提升。<br/>
      一个页面的“得票数”由所有链向它的页面的重要性来决定，到一个页面的超链接相当于对该页投一票。一个页面的PageRank是由所有链向它的页面（“链入页面”）的重要性经过递归算法得到的。一个有较多链入的页面会有较高的等级，相反如果一个页面没有任何链入页面，那么它没有等级。<br/>
**3. 利用链接文本** <br/> 
      根据指向某一网页的链接文本来决定网页的相关度。相比于被链接的网页自身所提供的信息而言，我们从指向该网页链接中所得到的信息会更有价值。<br/>

<font size=5>**从点击中学习**</font>  
    通过追踪用户的点击链接，可以推断出用户的喜好。由此我们可以构造一个**人工神经网络**，向其提供：查询条件中的单词，返回给用户的搜索结果，以及用户的点击决策，然后对其加以训练。<br/>
**1. 构建神经网络**  <br/>
    黑盒原理，对外只有输入和输出，内部则有多个隐藏层，对输入的数据进行处理。<br/>
**2. 内部处理方法**  <br/>
    - 前馈法  <br/>
    - 反向传播法  <br/>

# 第五章 优化
---

**1. 成本函数**  <br/>
    任何优化算法的目标，就是要寻找一组能够使成本函数的返回结果达到最小化的输入。<br/>
**2. 随机搜索**  <br/>
    一般数据处于无序，随机时，适用。<br/>
**3. 爬山法**  <br/>
    随机搜索的替代办法,以一个随机解开始，然后在起临近的解集中寻找更好的题解。<br/>
**4. 模拟退火算法**  <br/>
    以一个问题的随机解开始。它用一个变量来表示温度，这一温度开始时非常高，然后逐渐降低。在每一次迭代期间，算发挥随机选中题解中的某个数字，然后朝某个方向变化。**关键在于：**如果新的成本值更低，则新的题解就会被称为当前题解。这和爬山法非常相似。不过，如果成本值更高的话，则新的题解任将可能成为当前题解。<br/>
**5. 遗传算法** <br/> 
    这类算法的运行过程是先随机生成一组解，即种群。优化过程中的每一步，算法会计算整个种群的成本函数，从而得到一个有关题解的有序函数，即新的种群。<br/>
<font size=5>**涉及偏好的优化**</font><br/>
    尽可能让最优解的成本为零。<br/>

# 第六章 文档过滤
-----

**1. 文档和单词**  <br/>
即将构造的分类器需要利用某些特征来对不同的内容项进行分类。所谓特征，是指任何可以用来判断内容中具备或者缺失的东西。当考虑对文档进行分类时，所谓的内容是文档，而特征则是文档中的单词。当将单词作为特征时，其假设为：某些单次相对而言更有可能出现于垃圾信息中。<br/>
**2. 对分类器进行训练**  <br/>
**3. 计算概率**  <br/>
比如用一个单词在一篇属于某个分类的文档中出现的次数<br/>
<font size=5>**朴素分类器**</font><br/>
在求出指定单词在一篇属于某个分类的文档中出现的概率，就需要有一种办法将各个单词的概率进行组合，从而得出整片文档属于该分类的概率。

# 第七章 决策树建模
---
**1. 引入决策树**<br/>
决策树市一种更为简单的机器学习方法，它是对被观测数据(observations)进行分类的一种相当直观的方法，决策树在经过训练之后，看起来就像是以梳妆形式排列的一系列if-then语句。<br/>
**2. 对树进行训练**<br/>
本书采用分类回归树算法(CART)，首先创建一个根结点，然后通过评估表中的所有观测变量，从中找出最合适的变量对数据进行拆分。为此，算法考差了所有不同的变量，然后从中选出一个条件对结果数据进行分解。<br/>

